{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract text from html, pdf, txt, doc, xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle, json\n",
    "from tqdm import tqdm\n",
    "from os.path import join as pjoin\n",
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "import xml.etree.ElementTree as ET\n",
    "import docx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "from termcolor import cprint\n",
    "import logging\n",
    "import datetime\n",
    "\n",
    "cp = lambda msg,clr='green': cprint(msg, clr)\n",
    "\n",
    "# Set up logging\n",
    "def get_logger(log_file_name):\n",
    "    \"\"\"\n",
    "    Example:\n",
    "    logger = get_logger(\"Log file name\")        # Log file name is the log file's prefix\n",
    "    logger.info(\"info message\")\n",
    "    logger.warning(\"a warning\")\n",
    "    logger.error(\"error message\")\n",
    "    \"\"\"\n",
    "    now = datetime.datetime.now()\n",
    "    formatted_time = now.strftime(\"%Y-%m-%d_%I-%M-%S %p\")\n",
    "    os.makedirs(\"logs\", exist_ok=True)\n",
    "    logger = logging.getLogger(f'{__name__}_{log_file_name}')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    # create a file handler\n",
    "    handler = logging.FileHandler(f'logs/{log_file_name}_{formatted_time}.log')\n",
    "    handler.setLevel(logging.INFO)\n",
    "    # create a logging format\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    # add the handlers to the logger\n",
    "    logger.addHandler(handler)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extractor():\n",
    "    \"\"\"\n",
    "    Extract text from url (local or web)\n",
    "    File types: html, pdf, txt, docx, xml\n",
    "    Automatic encoding detection, extracting text in Unicode\n",
    "    \"\"\"\n",
    "    def __init__(self, logger=None):\n",
    "        self.logger = logger\n",
    "        self.north_sami_char_all = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'Á', 'á', 'Č', 'č', 'Đ', 'đ', 'Ŋ', 'ŋ', 'Š', 'š', 'Ŧ', 'ŧ', 'Ž', 'ž']\n",
    "        self.north_sami_char_additional = ['Á', 'á', 'Č', 'č', 'Đ', 'đ', 'Ŋ', 'ŋ', 'Š', 'š', 'Ŧ', 'ŧ', 'Ž', 'ž']\n",
    "    def _delete_lines_not_containing_chars(self, lines):\n",
    "        \"\"\" Delete lines not containing any North Sami characters\n",
    "            # lines: list of strings \"\"\"\n",
    "        char_list = self.north_sami_char_all\n",
    "        return [line for line in lines if any(char in line for char in char_list)]\n",
    "    def _normalize_spaces(self, string_list):\n",
    "        \"\"\" Replace one or more whitespace characters (\\s+) with a single space \"\"\"\n",
    "        return [re.sub(r'\\s+', ' ', string) for string in string_list]\n",
    "    def save_to_binary_file(self, data, file_path):\n",
    "        with open(file_path, 'wb') as file:\n",
    "            pickle.dump(data, file)\n",
    "    def load_from_binary_file(self, file_path):\n",
    "        with open(file_path, 'rb') as file:\n",
    "            return pickle.load(file)\n",
    "    def save_to_txt_file(self, text, file_path):\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(text)\n",
    "    def save_to_json_file(self, text, file_path):\n",
    "        with open(file_path, 'w') as file:\n",
    "            json.dump(text, file, ensure_ascii=False)\n",
    "    def extract_text(self, filetype, urls, texttype='json'):\n",
    "        if filetype == \".html\":\n",
    "            return self.extract_from_html(urls, texttype)\n",
    "        elif filetype == \".txt\":\n",
    "            return self.extract_from_txt(urls, texttype)\n",
    "        elif filetype == \".docx\":\n",
    "            return self.extract_from_docx(urls, texttype)\n",
    "        elif filetype == \".xml\":\n",
    "            return self.extract_from_xml(urls, texttype)\n",
    "        elif filetype == \".pdf\":\n",
    "            return self.extract_from_pdf(urls, texttype)\n",
    "    def _read_file(self, file_path, encoding='utf-8'):\n",
    "        with open(file_path, 'r', encoding=encoding, errors='ignore') as file:\n",
    "            html_content = file.read()\n",
    "        return html_content\n",
    "    def _extract_encoding(self, html_content, file_path):\n",
    "        # To find the charset in a meta tag\n",
    "        pattern = r'charset=[\"\\']?([\\w-]+)[\"\\']?'\n",
    "        match = re.search(pattern, html_content, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        else:\n",
    "            self.logger.error(f\"Could not find encoding in {file_path}\")\n",
    "            return 'utf-8'\n",
    "    def _read_file_with_detected_encoding(self, file_path):\n",
    "        \"\"\"\n",
    "        Detect file encoding by reading the charset in meta tag. If not found, use default utf-8 with errors ignored.\n",
    "        \"\"\"\n",
    "        html_content = self._read_file(file_path)\n",
    "        detected_encoding = self._extract_encoding(html_content, file_path)\n",
    "        html_content = self._read_file(file_path, encoding=detected_encoding)\n",
    "        unicode_content = html_content.encode(detected_encoding).decode(detected_encoding)      # converting to byte-iso-8859-1 then to unicode\n",
    "        return unicode_content, detected_encoding\n",
    "    def extract_from_html(self, urls, texttype):\n",
    "        all_url_json = []\n",
    "        all_text = \"\"\n",
    "        encoding_set = set()\n",
    "        for url in tqdm(urls):\n",
    "            try:\n",
    "                if os.path.isfile(url):     # local content\n",
    "                    content, detected_encoding = self._read_file_with_detected_encoding(url)\n",
    "                    encoding_set.add(detected_encoding)\n",
    "                else:       # web content\n",
    "                    import requests\n",
    "                    response = requests.get(url)\n",
    "                    content = response.content\n",
    "                soup = BeautifulSoup(content, 'html.parser')\n",
    "                # Remove all script and style elements\n",
    "                for script in soup([\"script\", \"style\"]):\n",
    "                    script.extract()\n",
    "                visible_text = soup.get_text()\n",
    "                paragraphs = visible_text.split('\\n')\n",
    "                # Remove empty paragraphs and strip leading/trailing spaces\n",
    "                paragraphs = [paragraph.strip() for paragraph in paragraphs if paragraph.strip()]\n",
    "                # Accumulate the paragraphs\n",
    "                paragraphs = self._delete_lines_not_containing_chars(paragraphs)\n",
    "                paragraphs = self._normalize_spaces(paragraphs)\n",
    "                all_url_json.append({\"url\": url, \"content\": paragraphs})\n",
    "                all_text += \"\\n\".join(paragraphs) + \"\\n\\n\"\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error in {url}: {e}\")\n",
    "                continue\n",
    "        if texttype == 'txt': return all_text, encoding_set\n",
    "        elif texttype == 'json': return all_url_json, encoding_set\n",
    "    def _join_broken_words(self, lines):\n",
    "        \"\"\"\n",
    "        Join words broken by hyphenation at end of line\n",
    "        \"\"\"\n",
    "        lines = [line for line in lines if line.strip()]\n",
    "        i = 0\n",
    "        while i < len(lines)-1:\n",
    "            if lines[i].endswith('-'):\n",
    "                split_words = lines[i+1].split()\n",
    "                lines[i] = lines[i][:-1] + split_words[0]\n",
    "                lines[i+1] = ' '.join(split_words[1:])\n",
    "            i += 1\n",
    "        # Remove empty lines\n",
    "        lines = [line for line in lines if line.strip()]\n",
    "        return lines\n",
    "    def extract_from_pdf(self, urls, texttype):\n",
    "        all_url_json = []\n",
    "        all_text = \"\"\n",
    "        for url in tqdm(urls):\n",
    "            try:\n",
    "                if os.path.isfile(url):\n",
    "                    doc = fitz.open(url)\n",
    "                else:\n",
    "                    raise FileNotFoundError(f\"No file found at {url}\")\n",
    "                url_full_text = []\n",
    "                for page in doc:\n",
    "                    blocks = page.get_text(\"blocks\")\n",
    "                    # https://pymupdf.readthedocs.io/en/latest/page.html#Page.get_text\n",
    "                    # https://pymupdf.readthedocs.io/en/latest/recipes-text.html\n",
    "                    for block in blocks:\n",
    "                        if not \"<image:\" in block[4]:   # ignore images\n",
    "                            text = block[4]             # 4th element contains text\n",
    "                            lines = text.split('\\n')\n",
    "                            lines = self._join_broken_words(lines)\n",
    "                            # Join all lines in a block\n",
    "                            block_text = ' '.join(line.strip() for line in lines)\n",
    "                            url_full_text.append(block_text)\n",
    "                url_full_text = self._delete_lines_not_containing_chars(url_full_text)\n",
    "                url_full_text = self._normalize_spaces(url_full_text)\n",
    "                all_text += \"\\n\".join(url_full_text) + \"\\n\\n\"\n",
    "                all_url_json.append({\"url\": url, \"content\": url_full_text})\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error in {url}: {e}\")\n",
    "                continue\n",
    "        if texttype == 'txt': return all_text, None\n",
    "        elif texttype == 'json': return all_url_json, None\n",
    "    def extract_from_xml(self, urls, texttype):\n",
    "        def _recursive_extract(element):\n",
    "            \"\"\" recursively extract text from each element \"\"\"\n",
    "            text = []\n",
    "            if element.text and element.text.strip():\n",
    "                text.append(element.text.strip())\n",
    "            for subelement in element:\n",
    "                text.extend(_recursive_extract(subelement))\n",
    "                if subelement.tail and subelement.tail.strip():\n",
    "                    text.append(subelement.tail.strip())\n",
    "            return text\n",
    "        all_url_json = []\n",
    "        all_text = \"\"\n",
    "        for url in tqdm(urls):\n",
    "            try:\n",
    "                if os.path.isfile(url):\n",
    "                    tree = ET.parse(url)\n",
    "                    root = tree.getroot()\n",
    "                else:\n",
    "                    raise FileNotFoundError(f\"No file found at {url}\")\n",
    "                url_full_text = _recursive_extract(root)\n",
    "                url_full_text = self._delete_lines_not_containing_chars(url_full_text)\n",
    "                url_full_text = self._normalize_spaces(url_full_text)\n",
    "                all_text += \"\\n\".join(url_full_text) + \"\\n\\n\"\n",
    "                all_url_json.append({\"url\": url, \"content\": url_full_text})\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error in {url}: {e}\")\n",
    "                continue\n",
    "        if texttype == 'txt': return all_text, None\n",
    "        elif texttype == 'json': return all_url_json, None\n",
    "    def extract_from_docx(self, urls, texttype):\n",
    "        all_url_json = []\n",
    "        all_text = \"\"\n",
    "        for url in tqdm(urls):\n",
    "            try:\n",
    "                if os.path.isfile(url):\n",
    "                    doc = docx.Document(url)\n",
    "                else:\n",
    "                    raise FileNotFoundError(f\"No file found at {url}\")\n",
    "                url_full_text = [paragraph.text.strip() for paragraph in doc.paragraphs if paragraph.text.strip()]\n",
    "                url_full_text = self._delete_lines_not_containing_chars(url_full_text)\n",
    "                url_full_text = self._normalize_spaces(url_full_text)\n",
    "                all_text += \"\\n\".join(url_full_text) + \"\\n\\n\"\n",
    "                all_url_json.append({\"url\": url, \"content\": url_full_text})\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error in {url}: {e}\")\n",
    "                continue\n",
    "        if texttype == 'txt': return all_text, None\n",
    "        elif texttype == 'json': return all_url_json, None\n",
    "    def extract_from_txt(self, urls, texttype):\n",
    "        all_url_json = []\n",
    "        all_text = \"\"\n",
    "        for url in tqdm(urls):\n",
    "            try:\n",
    "                if os.path.isfile(url):\n",
    "                    with open(url, 'r') as infile:\n",
    "                        url_full_text = [line.strip() for line in infile if line.strip()]\n",
    "                else:\n",
    "                    raise FileNotFoundError(f\"No file found at {url}\")\n",
    "                url_full_text = self._delete_lines_not_containing_chars(url_full_text)\n",
    "                url_full_text = self._normalize_spaces(url_full_text)\n",
    "                all_text += \"\\n\".join(url_full_text) + \"\\n\\n\"\n",
    "                all_url_json.append({\"url\": url, \"content\": url_full_text})\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error in {url}: {e}\")\n",
    "                continue\n",
    "        if texttype == 'txt': return all_text, None\n",
    "        elif texttype == 'json': return all_url_json, None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mExtracting .txt files from 3 b ...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 1935.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mExtracting .txt files from 6 a ...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 579.64it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "freecorpus:\n",
    "Extract data from all file types using the Extractor class\n",
    "\n",
    "\n",
    "\n",
    "Directions:\n",
    "- Below you will find 2 base dirs, the first set is for running on actual sami data, while the second set is for testing on a small set of files for debugging/correctness of code.\n",
    "\"\"\"\n",
    "\n",
    "logger = get_logger(\"TextExtraction\")\n",
    "ext = Extractor(logger)\n",
    "# base_read_dir = \"/media/peace/LaCie/samillm/data/a_scraped_raw_data/freecorpus\"\n",
    "# base_write_dir = \"/media/peace/LaCie/samillm/data/b_extracted_raw_data/freecorpus\"\n",
    "base_read_dir = \"/home/peace/Documents/UiT/OrganVision/Codes/g_common_code/c_samillm/src/a_datacollection/exp/read\"\n",
    "base_write_dir = \"/home/peace/Documents/UiT/OrganVision/Codes/g_common_code/c_samillm/src/a_datacollection/exp/write\"\n",
    "langs = os.listdir(base_read_dir)\n",
    "texttype = 'json'\n",
    "# file_types = ['.html']#, '.pdf', '.txt', '.xml', '.docx']\n",
    "# file_types = ['.pdf']#, , '.txt', '.xml', '.docx']\n",
    "# file_types = ['.xml']#, , '.txt', , '.docx']\n",
    "# file_types = ['.docx']#, , '.txt', , ]\n",
    "file_types = ['.txt']#, , , , ]\n",
    "for i,lang in enumerate(langs):\n",
    "    lang_dir = pjoin(base_read_dir, lang)\n",
    "    if not os.path.isdir(lang_dir):\n",
    "        continue\n",
    "    for file_type in file_types:\n",
    "        cp(f\"Extracting {file_type} files from {i+1} {lang} ...\")\n",
    "        files = os.listdir(lang_dir)\n",
    "        files = [file for file in files if file.endswith(file_type)]\n",
    "        urls = [pjoin(lang_dir, file) for file in files]\n",
    "        text, encoding_set = ext.extract_text(file_type, urls, texttype=texttype)\n",
    "        outfile_bin = pjoin(base_write_dir, lang, lang + file_type + '.bin')\n",
    "        outfile_txt = pjoin(base_write_dir, lang, lang + file_type + '.txt')\n",
    "        outfile_json = pjoin(base_write_dir, lang, lang + file_type + '.json')\n",
    "        os.makedirs(os.path.dirname(outfile_bin), exist_ok=True)\n",
    "        ext.save_to_binary_file(text, outfile_bin)\n",
    "        if texttype == 'txt': ext.save_to_txt_file(text, outfile_txt)\n",
    "        elif texttype == 'json': ext.save_to_json_file(text, outfile_json)\n",
    "        if encoding_set:\n",
    "            cp(f'Encoding set for {file_type}: {encoding_set}', 'red')\n",
    "            logger.info(f'Encoding set for {file_type}: {encoding_set}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "#####----- Deduplication : whole directory tree -----#####\n",
    "##########################################################\n",
    "\n",
    "RUNCODE = False\n",
    "\n",
    "import hashlib\n",
    "\n",
    "def remove_duplicate_lines_with_hashing(input_file_path, output_file_path):\n",
    "    seen_hashes = set()\n",
    "    output_lines = []\n",
    "\n",
    "    with open(input_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Create a hash of the line\n",
    "            line_hash = hashlib.sha256(line.encode()).hexdigest()\n",
    "\n",
    "            # If the hash is not in the seen_hashes, add it to the output\n",
    "            if line_hash not in seen_hashes:\n",
    "                seen_hashes.add(line_hash)\n",
    "                output_lines.append(line)\n",
    "\n",
    "    # Write the unique lines to the output file\n",
    "    with open(output_file_path, 'w') as file:\n",
    "        file.writelines(output_lines)\n",
    "\n",
    "if RUNCODE:\n",
    "    base_in_dir = '/media/peace/LaCie/samillm/data/b_extracted_raw_data/freecorpus'\n",
    "    base_out_dir = '/media/peace/LaCie/samillm/data/c_deduplicated'\n",
    "\n",
    "for root, dirs, files in tqdm(os.walk(base_in_dir)):\n",
    "    for file in files:\n",
    "        if file.endswith('.txt'):\n",
    "            input_file_path = os.path.join(root, file)\n",
    "            output_file_path = os.path.join(base_out_dir, file)\n",
    "            remove_duplicate_lines_with_hashing(input_file_path, output_file_path)\n",
    "            # print(f\"Removed duplicate lines from {input_file_path} and saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e1py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
